{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "Будем использовать логи прослушивания музыкальных исполнителей в сервисе Яндекс.Музыка.\n",
    "\n",
    "Файл `events.csv` содержит записи вида `Пользователь,Исполнитель,Число прослушиваний,Число пропусков`:\n",
    "```csv\n",
    "userId,artistId,plays,skips\n",
    "0,335,1,0\n",
    "0,708,1,0\n",
    "0,710,2,1\n",
    "0,815,1,1\n",
    "```\n",
    "\n",
    "Вам необходимо проделать следующее:\n",
    "1. **Оставьте в данных только тех пользователей, для которых сумма plays строго больше 1000. Сколько таких пользователей?**\n",
    "2. **В отфильтрованных на первом шаге данных найдите 5 самых популярных по числу пользователей исполнителей (идентификаторы).**\n",
    "\n",
    "Детали:\n",
    "1. Давайте считать, что список прослушиваний одного пользователя всегда помещается в память.\n",
    "\n",
    "Решение сохраните в файл `result.json`. Пример содержимого файла:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": 123,\n",
    "    \"q2\": [\n",
    "        4,\n",
    "        5,\n",
    "        6,\n",
    "        7,\n",
    "        8\n",
    "    ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,artistId,plays,skips\r\n",
      "0,335,1,0\r\n",
      "0,708,1,0\r\n",
      "0,710,2,1\r\n",
      "0,815,1,1\r\n"
     ]
    }
   ],
   "source": [
    "# пример содержимого файла\n",
    "! head -n 5 yandex_music/events.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat yandex_music/events.csv | tail -n +2 > yandex_music/events_no_header.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artists.jsonl  events.csv  events_no_header.csv  README.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls yandex_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 jovyan supergroup        254 2021-08-11 13:59 /yandex_music/README.txt\r\n",
      "-rw-r--r--   1 jovyan supergroup      3.7 M 2021-08-11 13:59 /yandex_music/artists.jsonl\r\n",
      "-rw-r--r--   1 jovyan supergroup     47.6 M 2021-08-11 13:59 /yandex_music/events.csv\r\n",
      "-rw-r--r--   1 jovyan supergroup     47.6 M 2021-08-11 13:59 /yandex_music/events_no_header.csv\r\n"
     ]
    }
   ],
   "source": [
    "# копируем файлы в HDFS\n",
    "! hadoop fs -copyFromLocal yandex_music /\n",
    "! hadoop fs -ls -h /yandex_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ТУТ ВАШ КОД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестируем локально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper.py\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    userId, _, plays, _ = line.split(',')\n",
    "    print(userId + \"\\t\" + plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer.py\n",
    "import sys\n",
    "\n",
    "prev_key = None\n",
    "count = 0\n",
    "for line in sys.stdin:  # stream is sorted by key\n",
    "    key, value = line.split(\"\\t\")\n",
    "    \n",
    "    if prev_key is not None and key != prev_key:\n",
    "        # new key in stream, dump previous\n",
    "        if count > 2000:\n",
    "            print(prev_key)\n",
    "        count = 0\n",
    "    \n",
    "    count += int(value)\n",
    "    prev_key = key\n",
    "\n",
    "# dump last key\n",
    "if count > 2000:\n",
    "    print(prev_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.6 ms, sys: 18.3 ms, total: 86.9 ms\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! cat yandex_music/events_no_header.csv | python ./mapper.py | python ./reducer.py > step1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1705\r\n"
     ]
    }
   ],
   "source": [
    "! cat step1.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /music-count\n",
      "2021-08-11 11:38:24,119 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper.py, reducer.py] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar] /tmp/streamjob5952599561469515453.jar tmpDir=null\n",
      "2021-08-11 11:38:24,915 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2021-08-11 11:38:25,072 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2021-08-11 11:38:25,251 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1628676009592_0004\n",
      "2021-08-11 11:38:25,492 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2021-08-11 11:38:25,522 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-08-11 11:38:25,616 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1628676009592_0004\n",
      "2021-08-11 11:38:25,616 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-08-11 11:38:25,783 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-08-11 11:38:25,783 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-08-11 11:38:25,834 INFO impl.YarnClientImpl: Submitted application application_1628676009592_0004\n",
      "2021-08-11 11:38:25,862 INFO mapreduce.Job: The url to track the job: http://7c91dd77d2de:8088/proxy/application_1628676009592_0004/\n",
      "2021-08-11 11:38:25,863 INFO mapreduce.Job: Running job: job_1628676009592_0004\n",
      "2021-08-11 11:38:30,920 INFO mapreduce.Job: Job job_1628676009592_0004 running in uber mode : false\n",
      "2021-08-11 11:38:30,921 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-08-11 11:38:43,100 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2021-08-11 11:38:44,103 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-08-11 11:38:52,128 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-08-11 11:38:52,133 INFO mapreduce.Job: Job job_1628676009592_0004 completed successfully\n",
      "2021-08-11 11:38:52,224 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=30125268\n",
      "\t\tFILE: Number of bytes written=61321185\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49919979\n",
      "\t\tHDFS: Number of bytes written=18069\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=16422400\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6630400\n",
      "\t\tTotal time spent by all map tasks (ms)=32075\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6475\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=32075\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6475\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16422400\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6630400\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3412504\n",
      "\t\tMap output records=3412504\n",
      "\t\tMap output bytes=23300254\n",
      "\t\tMap output materialized bytes=30125280\n",
      "\t\tInput split bytes=321\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4999\n",
      "\t\tReduce shuffle bytes=30125280\n",
      "\t\tReduce input records=3412504\n",
      "\t\tReduce output records=3117\n",
      "\t\tSpilled Records=6825008\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=733\n",
      "\t\tCPU time spent (ms)=12460\n",
      "\t\tPhysical memory (bytes) snapshot=1134510080\n",
      "\t\tVirtual memory (bytes) snapshot=8999604224\n",
      "\t\tTotal committed heap usage (bytes)=932184064\n",
      "\t\tPeak Map Physical memory (bytes)=339578880\n",
      "\t\tPeak Map Virtual memory (bytes)=2138398720\n",
      "\t\tPeak Reduce Physical memory (bytes)=192172032\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2585395200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49919658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=18069\n",
      "2021-08-11 11:38:52,224 INFO streaming.StreamJob: Output directory: /music-count\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /music-count\n",
    "\n",
    "! mapred streaming \\\n",
    "  -input /yandex_music/events_no_header.csv \\\n",
    "  -output /music-count \\\n",
    "  -mapper \"/opt/conda/bin/python3.6 mapper.py\" \\\n",
    "  -reducer \"/opt/conda/bin/python3.6 reducer.py\" \\\n",
    "  -file mapper.py \\\n",
    "  -file reducer.py \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3117\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /music-count/part-00000 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%file mapper2.py\n",
    "import sys\n",
    "with open('step1.txt', 'r') as myfile:\n",
    "    data = myfile.read().split()\n",
    "    \n",
    "for line in sys.stdin:\n",
    "    userId, artistId, _, _ = line.split(',')\n",
    "    if userId in data:\n",
    "        print(artistId + \"\\t\" + \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%file reducer2.py\n",
    "import sys\n",
    "\n",
    "prev_key = None\n",
    "count = 0\n",
    "for line in sys.stdin:  # stream is sorted by key\n",
    "    key, value = line.split(\"\\t\")\n",
    "    \n",
    "    if prev_key is not None and key != prev_key:\n",
    "        # new key in stream, dump previous\n",
    "        print(prev_key + \"\\t\" + str(count))\n",
    "        count = 0\n",
    "    \n",
    "    count += int(value)\n",
    "    prev_key = key\n",
    "\n",
    "# dump last key\n",
    "print(prev_key + \"\\t\" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /music-author\n",
      "2021-08-11 11:51:30,720 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper2.py, reducer2.py, step1.txt] [/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar] /tmp/streamjob8797366671545227778.jar tmpDir=null\n",
      "2021-08-11 11:51:31,510 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2021-08-11 11:51:31,654 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2021-08-11 11:51:31,828 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/jovyan/.staging/job_1628676009592_0008\n",
      "2021-08-11 11:51:32,059 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2021-08-11 11:51:32,495 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-08-11 11:51:32,581 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1628676009592_0008\n",
      "2021-08-11 11:51:32,581 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-08-11 11:51:32,721 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-08-11 11:51:32,722 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-08-11 11:51:32,774 INFO impl.YarnClientImpl: Submitted application application_1628676009592_0008\n",
      "2021-08-11 11:51:32,802 INFO mapreduce.Job: The url to track the job: http://7c91dd77d2de:8088/proxy/application_1628676009592_0008/\n",
      "2021-08-11 11:51:32,803 INFO mapreduce.Job: Running job: job_1628676009592_0008\n",
      "2021-08-11 11:51:38,861 INFO mapreduce.Job: Job job_1628676009592_0008 running in uber mode : false\n",
      "2021-08-11 11:51:38,861 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-08-11 11:51:56,978 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "2021-08-11 11:52:02,994 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "2021-08-11 11:52:09,010 INFO mapreduce.Job:  map 36% reduce 0%\n",
      "2021-08-11 11:52:15,069 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2021-08-11 11:52:21,084 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "2021-08-11 11:52:23,100 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2021-08-11 11:52:27,110 INFO mapreduce.Job:  map 65% reduce 0%\n",
      "2021-08-11 11:52:33,126 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "2021-08-11 11:52:39,142 INFO mapreduce.Job:  map 74% reduce 0%\n",
      "2021-08-11 11:52:41,148 INFO mapreduce.Job:  map 74% reduce 11%\n",
      "2021-08-11 11:52:42,151 INFO mapreduce.Job:  map 86% reduce 11%\n",
      "2021-08-11 11:52:45,158 INFO mapreduce.Job:  map 88% reduce 11%\n",
      "2021-08-11 11:52:47,163 INFO mapreduce.Job:  map 88% reduce 22%\n",
      "2021-08-11 11:52:49,168 INFO mapreduce.Job:  map 100% reduce 22%\n",
      "2021-08-11 11:52:52,176 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-08-11 11:52:53,181 INFO mapreduce.Job: Job job_1628676009592_0008 completed successfully\n",
      "2021-08-11 11:52:53,259 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=25031843\n",
      "\t\tFILE: Number of bytes written=51135495\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49919979\n",
      "\t\tHDFS: Number of bytes written=427596\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=87732736\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=27018240\n",
      "\t\tTotal time spent by all map tasks (ms)=171353\n",
      "\t\tTotal time spent by all reduce tasks (ms)=26385\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=171353\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=26385\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=87732736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=27018240\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3412504\n",
      "\t\tMap output records=2560216\n",
      "\t\tMap output bytes=19911405\n",
      "\t\tMap output materialized bytes=25031855\n",
      "\t\tInput split bytes=321\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=51038\n",
      "\t\tReduce shuffle bytes=25031855\n",
      "\t\tReduce input records=2560216\n",
      "\t\tReduce output records=51038\n",
      "\t\tSpilled Records=5120432\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=588\n",
      "\t\tCPU time spent (ms)=112420\n",
      "\t\tPhysical memory (bytes) snapshot=1046614016\n",
      "\t\tVirtual memory (bytes) snapshot=8999776256\n",
      "\t\tTotal committed heap usage (bytes)=808452096\n",
      "\t\tPeak Map Physical memory (bytes)=304320512\n",
      "\t\tPeak Map Virtual memory (bytes)=2166534144\n",
      "\t\tPeak Reduce Physical memory (bytes)=175435776\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2588180480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49919658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=427596\n",
      "2021-08-11 11:52:53,259 INFO streaming.StreamJob: Output directory: /music-author\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /music-author\n",
    "\n",
    "! mapred streaming \\\n",
    "  -input /yandex_music/events_no_header.csv \\\n",
    "  -output /music-author \\\n",
    "  -mapper \"/opt/conda/bin/python3.6 mapper2.py\" \\\n",
    "  -reducer \"/opt/conda/bin/python3.6 reducer2.py\" \\\n",
    "  -file mapper2.py \\\n",
    "  -file reducer2.py \\\n",
    "  -file step1.txt \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11368\t2574\r\n",
      "3629\t2286\r\n",
      "259\t2208\r\n",
      "44148\t2161\r\n",
      "23524\t2110\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /music-author/part-00000 | sort -k2 -n -r | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing result.json\n"
     ]
    }
   ],
   "source": [
    "%%file result.json\n",
    "{\n",
    "    \"q1\": 3117,\n",
    "    \"q2\": [\n",
    "        11368,\n",
    "        3629,\n",
    "        259,\n",
    "        44148,\n",
    "        23524\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"q1\": 3117,\r\n",
      "    \"q2\": [\r\n",
      "        11368,\r\n",
      "        3629,\r\n",
      "        259,\r\n",
      "        44148,\r\n",
      "        23524\r\n",
      "    ]\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat result.json"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-3-mapreduce"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
