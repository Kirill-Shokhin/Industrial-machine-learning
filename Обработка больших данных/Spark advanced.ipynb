{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"../cluster\" style=\"font-size:20px\">All Applications (YARN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "Давайте рассмотрим задачу классификации на следующие два класса:\n",
    "- 1 для 'American_movie_actors'\n",
    "- 0 для 'American_stage_actors'\n",
    "\n",
    "На лекциях мы обсуждали, что вместо словаря можно использовать хэширование.\n",
    "\n",
    "Вам предлагается проверить, как поведет себя модель после использования хэширования и ответить на следующие вопросы:\n",
    "1. **Какой roc_auc_score на тестовой выборке получается при использовании словаря?**\n",
    "2. **Какой roc_auc_score на тестовой выборке получается при переходе со словаря на хэширование?**\n",
    "\n",
    "Детали:\n",
    "1. Разбейте выборки на обучающую и тестовую по четности `id` статьи: четные в обучение, нечетные в тест. Только по тренировочной части мы считаем градиенты!\n",
    "2. Для подсчета roc_auc_score вам нужно получить предсказания и истинные ответы для примеров из тестовой выборки. Все пары (предсказание, ответ) помещаются в память, воспользуйтесь этим!\n",
    "3. В качестве хэш-функции используйте `murmurhash3_32(x) % 2**20`.\n",
    "4. Зафиксируйте random seed в начальном приближении весов: `np.random.seed(0); weights = np.random.random(...)`\n",
    "5. Обучите 500 эпох с шагом 0.3. После каждой эпохи вызывайте `weights_broadcast.destroy()` для удаления broadcast переменной, чтобы не закончилась память. \n",
    "6. Вот так выглядит roc_auc_score на тестовой выборке от числа эпох (чем больше roc_auc_score, тем лучше):\n",
    "<img src=\"images/test_auc.png\" width=\"600px\"></img>\n",
    "\n",
    "Решение сохраните в файл `result.json`. Пример содержимого файла:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"q1\": 0.123,\n",
    "    \"q2\": 0.456\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import murmurhash3_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# y_true - настоящие классы\n",
    "# y_score - вероятности класса 1\n",
    "# https://ru.wikipedia.org/wiki/ROC-кривая\n",
    "roc_auc_score(y_true=[1, 1, 0, 0], y_score=[0.8, 0.7, 0.3, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ТУТ ВАШЕ РЕШЕНИЕ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение через мешок слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Копируем файлы в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/wiki/wiki.jsonl': File exists\r\n",
      "copyFromLocal: `/wiki/README.txt': File exists\r\n",
      "copyFromLocal: `/wiki/categories.jsonl': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal wiki /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   1 jovyan supergroup        387 2021-08-23 10:58 /wiki/README.txt\r\n",
      "-rw-r--r--   1 jovyan supergroup     60.9 M 2021-08-23 10:58 /wiki/categories.jsonl\r\n",
      "-rw-r--r--   1 jovyan supergroup    143.4 M 2021-08-23 10:58 /wiki/wiki.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls -h /wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготавливаем таблицу для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>April\\n\\nApril is the fourth month of the year...</td>\n",
       "      <td>April</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August\\n\\nAugust (Aug.) is the eighth month of...</td>\n",
       "      <td>August</td>\n",
       "      <td>https://simple.wikipedia.org/wiki?curid=2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                               text   title  \\\n",
       "0  1  April\\n\\nApril is the fourth month of the year...   April   \n",
       "1  2  August\\n\\nAugust (Aug.) is the eighth month of...  August   \n",
       "\n",
       "                                         url  \n",
       "0  https://simple.wikipedia.org/wiki?curid=1  \n",
       "1  https://simple.wikipedia.org/wiki?curid=2  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем обе таблицы в dataframe\n",
    "wiki = se.read.json(\"hdfs:///wiki/wiki.jsonl\")\n",
    "wiki.registerTempTable(\"wiki\")\n",
    "wiki.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>page_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Months</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Months</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  page_id\n",
       "0   Months        1\n",
       "1   Months        2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = se.read.json(\"hdfs:///wiki/categories.jsonl\")\n",
    "categories.registerTempTable(\"categories\")\n",
    "categories.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alanis Morissette\\n\\nAlanis Nadine Morissette ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ronald Reagan\\n\\nRonald Wilson Reagan (; Febru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gwyneth Paltrow\\n\\nGwyneth Kate Paltrow (born ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gwyneth Paltrow\\n\\nGwyneth Kate Paltrow (born ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heather Graham\\n\\nHeather Joan Graham (born Ja...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Alanis Morissette\\n\\nAlanis Nadine Morissette ...       1\n",
       "1  Ronald Reagan\\n\\nRonald Wilson Reagan (; Febru...       1\n",
       "2  Gwyneth Paltrow\\n\\nGwyneth Kate Paltrow (born ...       0\n",
       "3  Gwyneth Paltrow\\n\\nGwyneth Kate Paltrow (born ...       1\n",
       "4  Heather Graham\\n\\nHeather Joan Graham (born Ja...       1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = se.sql(\"\"\"\n",
    "select \n",
    "    wiki.text,\n",
    "    cast(categories.category == 'American_movie_actors' as int) as target\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "where (categories.category in ('American_movie_actors', 'American_stage_actors')) and (id % 2 = 0)\n",
    "order by cast(id as int)\n",
    "\"\"\")\n",
    "train_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Britney Spears\\n\\nBritney Jean Spears (born De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sarah Michelle Gellar\\n\\nSarah Michelle Gellar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jennifer Garner\\n\\nJenny Anne Garner (born Apr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christina Ricci\\n\\nChristina Ricci (born Febru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angelina Jolie\\n\\nAngelina Jolie (; née Voight...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Britney Spears\\n\\nBritney Jean Spears (born De...       1\n",
       "1  Sarah Michelle Gellar\\n\\nSarah Michelle Gellar...       1\n",
       "2  Jennifer Garner\\n\\nJenny Anne Garner (born Apr...       1\n",
       "3  Christina Ricci\\n\\nChristina Ricci (born Febru...       1\n",
       "4  Angelina Jolie\\n\\nAngelina Jolie (; née Voight...       1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = se.sql(\"\"\"\n",
    "select \n",
    "    wiki.text,\n",
    "    cast(categories.category == 'American_movie_actors' as int) as target\n",
    "from\n",
    "    wiki join categories on wiki.id == categories.page_id\n",
    "where (categories.category in ('American_movie_actors', 'American_stage_actors')) and (id % 2 != 0)\n",
    "order by cast(id as int)\n",
    "\"\"\")\n",
    "test_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Топ 50000 слов по всему wiki (задача WordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(f'[^{re.escape(string.printable)}]', ' ', text)  # непечатные символы заменяем на пробел\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)  # и пунктуацию\n",
    "    words = text.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def mapper(line):\n",
    "    text = json.loads(line)['text']\n",
    "    words = tokenize(text)\n",
    "    return [(word, 1) for word in set(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# алгоритм MapReduce\n",
    "word_counts = (\n",
    "    sc.textFile(\"hdfs:///wiki/wiki.jsonl\")\n",
    "    .flatMap(mapper)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('out', 10522),\n",
       " ('start', 2844),\n",
       " ('where', 12648),\n",
       " ('starts', 1369),\n",
       " ('exactly', 701)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_counts = sorted(word_counts, key=lambda x: -x[1])[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 143585),\n",
       " ('in', 134975),\n",
       " ('a', 134315),\n",
       " ('of', 128899),\n",
       " ('is', 125063)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_word_counts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизуем тексты (модель \"мешка слов\") и заливаем в кэш"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, (word, count) in enumerate(word_counts)}\n",
    "word_to_index_broadcast = sc.broadcast(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(row):\n",
    "    words = tokenize(row.text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index_broadcast.value:\n",
    "            index = word_to_index_broadcast.value[word]\n",
    "            indices.append(index)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values), row.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1538] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_data.rdd.map(mapper)\n",
    "test = test_data.rdd.map(mapper)\n",
    "\n",
    "train.cache()\n",
    "test.cache()     #unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество примеров\n",
    "len_test = test.count()\n",
    "len_train = train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучаем логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / (1. + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(weights_broadcast, loss, examples):\n",
    "    # здесь накапливаем вклад в градиент\n",
    "    gradient = np.zeros(len(weights_broadcast.value))\n",
    "    \n",
    "    for example in examples:\n",
    "        indices, values, target = example\n",
    "\n",
    "        # делаем предсказание с текущими весами\n",
    "        p = sigmoid(values.dot(weights_broadcast.value[indices]))\n",
    "\n",
    "        # добавляем в накопитель градиента\n",
    "        gradient[indices] += values * (p - target)\n",
    "\n",
    "        # считаем потери\n",
    "        p = np.clip(p, 1e-15, 1-1e-15)\n",
    "        loss.add(-(target * np.log(p) + (1 - target) * np.log(1 - p)))\n",
    "    \n",
    "    yield gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.61409150473\n",
      "epoch: 50 loss: 0.553655212024\n",
      "epoch: 100 loss: 0.538167106094\n",
      "epoch: 150 loss: 0.527261777515\n",
      "epoch: 200 loss: 0.518641929089\n",
      "epoch: 250 loss: 0.511448695228\n",
      "epoch: 300 loss: 0.505244909752\n",
      "epoch: 350 loss: 0.499775344419\n",
      "epoch: 400 loss: 0.494877553433\n",
      "epoch: 450 loss: 0.490441197741\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "# случайные веса\n",
    "\n",
    "np.random.seed(0); weights = np.random.random(len(word_to_index))\n",
    "\n",
    "# эпохи градиентного спуска\n",
    "for i in range(500):\n",
    "    weights_broadcast = sc.broadcast(weights)\n",
    "    loss = sc.accumulator(0.0)\n",
    "    \n",
    "    # считаем градиент\n",
    "    gradient = (\n",
    "        train\n",
    "        .coalesce(2)  # склеиваем 200 кэшированных партиций в 2\n",
    "        .mapPartitions(partial(compute_gradient, weights_broadcast, loss))\n",
    "        .reduce(lambda a, b: a + b)\n",
    "    )\n",
    "\n",
    "    # обновляем веса\n",
    "    weights -= 0.3 * gradient\n",
    "    \n",
    "    weights_broadcast.destroy()\n",
    "    if i % 50 == 0:\n",
    "        print(\"epoch:\", i, \"loss:\", loss.value / len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weights, examples):\n",
    "    p = []\n",
    "    targ = []\n",
    "    for example in examples:\n",
    "        indices, values, target = example\n",
    "\n",
    "        # делаем предсказание с текущими весами\n",
    "        p.append(sigmoid(values.dot(weights[indices])))\n",
    "        targ.append(target)\n",
    "    yield p, targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    test\n",
    "    .coalesce(2)  # склеиваем 200 кэшированных партиций в 2\n",
    "    .mapPartitions(partial(predict, weights))\n",
    ") .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = np.concatenate(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68784622476892943"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer1 = roc_auc_score(y_true=y_true, y_score=y_pred)\n",
    "answer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение через хэширование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(row):\n",
    "    words = tokenize(row.text)\n",
    "    indices = []\n",
    "    values = []\n",
    "    for word, count in Counter(words).items():\n",
    "        if word in word_to_index:\n",
    "            index = word_to_index[word]\n",
    "            hash_ = murmurhash3_32(index) % 2**20\n",
    "            indices.append(hash_)\n",
    "            tf = count / float(len(words))\n",
    "            values.append(tf)\n",
    "    return np.array(indices), np.array(values), row.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2550] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1 = train_data.rdd.map(mapper1)\n",
    "test1 = test_data.rdd.map(mapper1)\n",
    "\n",
    "train1.cache()\n",
    "test1.cache()     #unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_test1 = test1.count()\n",
    "len_train1 = train1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 0.617751103075\n",
      "epoch: 50 loss: 0.553437494655\n",
      "epoch: 100 loss: 0.538050174999\n",
      "epoch: 150 loss: 0.527202030713\n",
      "epoch: 200 loss: 0.518620375258\n",
      "epoch: 250 loss: 0.511455428773\n",
      "epoch: 300 loss: 0.505274070658\n",
      "epoch: 350 loss: 0.499823051273\n",
      "epoch: 400 loss: 0.494940977543\n",
      "epoch: 450 loss: 0.490518128375\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "# случайные веса\n",
    "np.random.seed(0)\n",
    "weights = np.random.random(2**20)\n",
    "\n",
    "# эпохи градиентного спуска\n",
    "for i in range(500):\n",
    "    weights_broadcast = sc.broadcast(weights)\n",
    "    loss = sc.accumulator(0.0)\n",
    "    \n",
    "    \n",
    "    # считаем градиент\n",
    "    gradient = (\n",
    "        train1\n",
    "        .coalesce(2)  # склеиваем 200 кэшированных партиций в 2\n",
    "        .mapPartitions(partial(compute_gradient, weights_broadcast, loss))\n",
    "        .reduce(lambda a, b: a + b)\n",
    "    )\n",
    "\n",
    "    # обновляем веса\n",
    "    weights -= 0.3 * gradient\n",
    "    \n",
    "    weights_broadcast.destroy()\n",
    "    if i % 50 == 0:\n",
    "        print(\"epoch:\", i, \"loss:\", loss.value / len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = (\n",
    "    test1\n",
    "    .coalesce(2)  # склеиваем 200 кэшированных партиций в 2\n",
    "    .mapPartitions(partial(predict, weights))\n",
    ") .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = np.concatenate(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68784622476892943"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer2 = roc_auc_score(y_true=y_true, y_score=y_pred)\n",
    "answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer1 == answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting result.json\n"
     ]
    }
   ],
   "source": [
    "%%file result.json\n",
    "{\n",
    "    \"q1\": 0.68784622476892943,\n",
    "    \"q2\": 0.68784622476892943\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"q1\": 0.68784622476892943,\r\n",
      "    \"q2\": 0.68784622476892943\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# останавливаем Spark (и YARN приложение)\n",
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-5-advanced-spark-aabb"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
